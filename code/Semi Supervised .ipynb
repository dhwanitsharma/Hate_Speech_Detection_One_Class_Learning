{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. 1917494  words loaded!\n"
     ]
    }
   ],
   "source": [
    "#5300 dimension Glove embeddings\n",
    "import numpy as np\n",
    "gloveFile = 'glove.42B.300d.txt'\n",
    "with open(gloveFile, encoding=\"utf8\" ) as f:\n",
    "    #content = f.readlines()\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split(' ')\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "print (\"Done.\",len(model),\" words loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_pickle('train_df.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.sample(frac = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['text_split'] = train_df['text'].str.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57419"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set([item for x in train_df['text_split'] for item in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words= 30000)\n",
    "tokenizer.fit_on_texts(train_df['text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((30000, 300))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    if index > 30000 - 1:\n",
    "        break\n",
    "    else:\n",
    "        if word in model.keys():\n",
    "            embedding_matrix[index] = model[word]  \n",
    "        else:\n",
    "            embedding_matrix[index] = model['unk']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pu(df,frac):\n",
    "    positive = df[df['label']==1].sample(n = frac)\n",
    "    negative = df[df['label']==0].sample(n = frac)\n",
    "    print(positive)\n",
    "    selected_examples = positive['comment_id'].values+ negative['comment_id'].values\n",
    "    \n",
    "    unlabeled = df[~((df['comment_id'].isin(selected_examples)))]\n",
    "    total = pd.concat([positive,negative])\n",
    "    total = total.sample(frac = 1)\n",
    "    X = total['text'].values\n",
    "    y = total['label'].values\n",
    "    \n",
    "    \n",
    "\n",
    "    return X,y,unlabeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       comment_id                                               text  \\\n",
      "6072        16765                       Grab em by the pussy mah boi   \n",
      "22068       38223  Boys like this are literal trash and are the m...   \n",
      "18131       44521  No place on Broadway for jihadi Muslims like t...   \n",
      "50708          66  Get ready to be flogged India is ready to flog...   \n",
      "41370        9993  rBPT Fuck America holding its own people at gu...   \n",
      "...           ...                                                ...   \n",
      "79210       28053        @Speech7x7 Deportation is the only cure URL   \n",
      "3227        15757                           I hope he raped her hard   \n",
      "14416       14179                                        white trash   \n",
      "2439         1678                        yeah fuck that stupid whore   \n",
      "31976         441  Hey Petie kill yourself   EDIT Lol @ all the S...   \n",
      "\n",
      "       hate_speech_score  label  \\\n",
      "6072                1.72      1   \n",
      "22068               0.62      1   \n",
      "18131               0.58      1   \n",
      "50708               1.99      1   \n",
      "41370               0.69      1   \n",
      "...                  ...    ...   \n",
      "79210               0.92      1   \n",
      "3227                2.19      1   \n",
      "14416               0.44      1   \n",
      "2439                1.24      1   \n",
      "31976               2.72      1   \n",
      "\n",
      "                                              text_split  \n",
      "6072                [Grab, em, by, the, pussy, mah, boi]  \n",
      "22068  [Boys, like, this, are, literal, trash, and, a...  \n",
      "18131  [No, place, on, Broadway, for, jihadi, Muslims...  \n",
      "50708  [Get, ready, to, be, flogged, India, is, ready...  \n",
      "41370  [rBPT, Fuck, America, holding, its, own, peopl...  \n",
      "...                                                  ...  \n",
      "79210  [@Speech7x7, Deportation, is, the, only, cure,...  \n",
      "3227                     [I, hope, he, raped, her, hard]  \n",
      "14416                                     [white, trash]  \n",
      "2439                   [yeah, fuck, that, stupid, whore]  \n",
      "31976  [Hey, Petie, kill, yourself, , , EDIT, Lol, @,...  \n",
      "\n",
      "[6117 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "X,y,unlabeled = make_pu(train_df,6117)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tokenizer.texts_to_sequences(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pad_sequences(X, padding = 'pre', maxlen = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,   32,\n",
       "          1,  953,   45,  503, 4102, 1977,   48])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_glove = Sequential()\n",
    "model_glove.add(Embedding(30000,300, input_length=128, weights=[embedding_matrix], trainable=False))\n",
    "model_glove.add(Dropout(0.2))\n",
    "model_glove.add(LSTM(32))\n",
    "model_glove.add(Dense(1, activation='sigmoid'))\n",
    "opt = Adam(learning_rate = 0.01)\n",
    "model_glove.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor = 'val_loss', patience = 4,restore_best_weights =True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "153/153 [==============================] - 3s 20ms/step - loss: 0.5303 - accuracy: 0.7348 - val_loss: 0.4996 - val_accuracy: 0.7515\n",
      "Epoch 2/50\n",
      "153/153 [==============================] - 2s 16ms/step - loss: 0.4642 - accuracy: 0.7726 - val_loss: 0.4935 - val_accuracy: 0.7544\n",
      "Epoch 3/50\n",
      "153/153 [==============================] - 2s 16ms/step - loss: 0.4282 - accuracy: 0.7953 - val_loss: 0.5040 - val_accuracy: 0.7348\n",
      "Epoch 4/50\n",
      "153/153 [==============================] - 3s 17ms/step - loss: 0.3973 - accuracy: 0.8145 - val_loss: 0.5293 - val_accuracy: 0.7348\n",
      "Epoch 5/50\n",
      "153/153 [==============================] - 3s 17ms/step - loss: 0.3627 - accuracy: 0.8375 - val_loss: 0.5388 - val_accuracy: 0.7515\n",
      "Epoch 6/50\n",
      "153/153 [==============================] - 3s 17ms/step - loss: 0.3273 - accuracy: 0.8581 - val_loss: 0.5794 - val_accuracy: 0.7532\n"
     ]
    }
   ],
   "source": [
    "history = model_glove.fit(X_train,y_train,validation_data=(X_val,y_val),epochs=50,batch_size=64,callbacks = [early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = pd.read_pickle('test_df.pickle').sample(frac = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pad_sequences(tokenizer.texts_to_sequences(test_ds['text'].values),  padding = 'pre', maxlen = 128)\n",
    "y_test = test_ds['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM before EM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.70      0.75      2000\n",
      "           1       0.73      0.83      0.78      2000\n",
      "\n",
      "    accuracy                           0.76      4000\n",
      "   macro avg       0.77      0.76      0.76      4000\n",
      "weighted avg       0.77      0.76      0.76      4000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model_glove.predict_classes(X_test)\n",
    "print(\"LSTM before EM\")\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_indices = unlabeled['comment_id']\n",
    "unlabeled_text = pad_sequences(tokenizer.texts_to_sequences(unlabeled['text'].values),  padding = 'pre', maxlen = 128)\n",
    "confidence_list = model_glove.predict(unlabeled_text)\n",
    "confidence_list[:5]\n",
    "\n",
    "confidence_list = np.array([i[0] for i in confidence_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_confident_negative = np.where(confidence_list<=0.05 )\n",
    "most_confident_positive= np.where(confidence_list>=.95 )\n",
    "x = pd.DataFrame(columns = ['elements','label'])\n",
    "x['elements'] = most_confident_positive[0]\n",
    "x = x.reset_index(drop = True)\n",
    "x['label'] = 1\n",
    "x\n",
    "x2= pd.DataFrame(columns = ['elements','label'])\n",
    "x2['elements'] = most_confident_negative[0]\n",
    "x2 = x2.reset_index(drop = True)\n",
    "x2['label'] = 0\n",
    "x3= pd.concat([x,x2])\n",
    "\n",
    "x3 = x3.sample(frac= 1)\n",
    "\n",
    "confident_indices = x3['elements'].values\n",
    "\n",
    "X_conf = [unlabeled_text[i] for i in confident_indices]\n",
    "\n",
    "y_conf = x3['label'].values\n",
    "\n",
    "mask = np.ones(len(unlabeled_text), dtype=bool)\n",
    "mask[confident_indices] = False\n",
    "unlabeled_text = unlabeled_text[mask]\n",
    "X_new =np.concatenate((np.array(X_conf),X_train),axis=0)\n",
    "y_new = np.concatenate((np.array(y_conf),y_train),axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "218/218 [==============================] - 4s 18ms/step - loss: 0.4224 - accuracy: 0.7976 - val_loss: 0.5093 - val_accuracy: 0.7462\n",
      "Epoch 2/50\n",
      "218/218 [==============================] - 4s 18ms/step - loss: 0.3532 - accuracy: 0.8332 - val_loss: 0.4930 - val_accuracy: 0.7487\n",
      "Epoch 3/50\n",
      "218/218 [==============================] - 4s 18ms/step - loss: 0.3366 - accuracy: 0.8394 - val_loss: 0.5626 - val_accuracy: 0.7511\n",
      "Epoch 4/50\n",
      "218/218 [==============================] - 4s 19ms/step - loss: 0.3260 - accuracy: 0.8447 - val_loss: 0.4899 - val_accuracy: 0.7540\n",
      "Epoch 5/50\n",
      "218/218 [==============================] - 4s 17ms/step - loss: 0.3125 - accuracy: 0.8533 - val_loss: 0.5059 - val_accuracy: 0.7564\n",
      "Epoch 6/50\n",
      "218/218 [==============================] - 4s 18ms/step - loss: 0.2976 - accuracy: 0.8613 - val_loss: 0.5078 - val_accuracy: 0.7577\n",
      "Epoch 7/50\n",
      "218/218 [==============================] - 4s 18ms/step - loss: 0.2899 - accuracy: 0.8659 - val_loss: 0.5002 - val_accuracy: 0.7536\n",
      "Epoch 8/50\n",
      "218/218 [==============================] - 4s 18ms/step - loss: 0.2787 - accuracy: 0.8717 - val_loss: 0.5329 - val_accuracy: 0.7528loss: - ETA: 2s - los\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x294d2079c88>"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#REtraining \n",
    "model_glove = Sequential()\n",
    "model_glove.add(Embedding(30000,300, input_length=128, weights=[embedding_matrix], trainable=False))\n",
    "model_glove.add(Dropout(0.2))\n",
    "model_glove.add(LSTM(32))\n",
    "model_glove.add(Dense(1, activation='sigmoid'))\n",
    "new_opt = Adam(learning_rate = 0.001)\n",
    "model_glove.compile(loss='binary_crossentropy', optimizer=new_opt, metrics=['accuracy'])\n",
    "\n",
    "model_glove.fit(X_new,y_new,epochs=50,validation_data=(X_val,y_val),batch_size=64,callbacks = [early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM after EM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.73      0.75      2000\n",
      "           1       0.75      0.79      0.77      2000\n",
      "\n",
      "    accuracy                           0.76      4000\n",
      "   macro avg       0.76      0.76      0.76      4000\n",
      "weighted avg       0.76      0.76      0.76      4000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model_glove.predict_classes(X_test)\n",
    "print(\"LSTM after EM\")\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
